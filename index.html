<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <link rel="stylesheet" href="./public/css/styles.css">
</head>
<body>
<p class="contentc">Elon Musk just shared the news about Grok, a big AI that's like the "Hitchhiker’s Guide to the Galaxy." It’s meant to not just answer questions but also help us figure out what to ask. Cool, right?</p>

<p class="contentc">Grok is joining X, the new name for Twitter. It’s all about answering questions with a funny twist. They’re even saying, "Don’t use Grok if you don’t like humor!" It’s going to be a fun ride!</p>

<img class="post-imagec" src="https://pbs.twimg.com/card_img/1721347239036219392/ZGbRGQJY?format=jpg&name=small" alt="">

<p class="contentc">Grok, through X, has the latest info and can handle even those spicy questions others might dodge. It’s a pretty new AI, only four months old, trained for two months. Still in beta, but they promise it’s going to get even better soon.</p>

<h4 class="titlec">Purpose of Grok</h4>
<p class="contentc">Grok’s here to help us humans understand and learn things better. It’s backed by Grok-1 LLM, made in four months. The first version, Grok-0, trained with 33 billion parameters, is as good as Meta’s LLaMA 2, which uses 70 billion parameters.</p>

<h4 class="titlec">Grok Capabilities</h4>
<p class="contentc">Grok-1 does pretty well: it hits 63.2% on the HumanEval coding task and 73% on MMLU. Even though it's not GPT-4 level, xAI's pumped about how much they've boosted Grok-1 compared to Grok-0 in a short time.</p>

<p class="contentc">Grok-1’s doing pretty good on the GSM8k math problems—62.9%, higher than GPT-3.5 and LLaMa 2 but not as high as Palm 2, Claude 2, or GPT-4. It’s getting there!</p>

<p class="contentc">Grok-1's holding its ground across various benchmarks: MMLU, HumanEval, Python code generation, and even middle and high school math tests in LaTeX. It might not be leading the pack, but it’s making a good showing!</p>

<p class="contentc">Grok-1’s been hand-graded and did decently in the 2023 Hungarian national high school math finals, hitting a C grade at 59%, beating Claude 2 but falling short of GPT-4, which scored a B at 68%. It’s better than GPT-3.5 but not yet at GPT-4 level. xAI's confident: even with less training data, Grok-1’s holding its own against models needing more data and heavier computing power.</p>

<p class="contentc">Grok-1’s got a unique setup for training and working, using Kubernetes, Rust, and JAX. With its internet access for real-time info, the company points out something important: it could potentially generate false or conflicting info.</p>

<p class="contentc">xAI’s on it for the next models. They’re after human input, better context understanding, handling different types of info, and making sure the AI can handle tricky situations well. They’re aiming for robustness against tricky stuff.</p>

<p class="contentc">Right now, only a few folks in the US can try out the beta Grok. But soon, it’s headed to X Premium+ subscribers for about Rs 1,300 a month if you sign up from a desktop. It’s getting closer!</p>
</body>
</html>